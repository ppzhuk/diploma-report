%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Анализ методов извлечения часто задаваемых вопросов}
\label{chap:overview}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

В данной главе рассмотрены подходы к задаче извлечения ВОП, определен наиболее эффективный из подходов. Дается определение процессу тематического моделирования, вводится понятие тематической модели. Рассмотрены различные тематические модели.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Существующие подходы к задаче извлечения ЧЗВ}
\label{sec:researches}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Предварительным этапом данной работы стало изучение существующих методик извлечения вопросно-ответных пар. Были изучены различные научные статьи за период 2004--2016 гг., выявлены следующие методы:

\begin{itemize}
\item Методы, основанные на машинном обучении~---~\cite{ML1},~\cite{ML2};
\item Методы, основанные на классификации~---~\cite{classif1},~\cite{engine};
\item Методы на основе тематического моделирования~---~\cite{LDA1},~\cite{LDA2},~\cite{original}.
\end{itemize}

Методы, применяющие тематическое моделирование, являются наиболее предпочтительными, поскольку построение и использование тематчиеской модели позволяет находить ВОП, у которых в вопросе и ответе используется схожая терминология. Это позволяет находить более качественные ВОП~\cite{LDA2} по сравнению со способами, использующими машинное обучение или классификацию.

Работа~\cite{original} предлагает решать задачу извлечения ВОП в три шага: предобработка данных, тематическое моделирование, поиск вопросно-ответных пар. От других работ, использующих тематическое моделирование, эту работу отличает наличие дополнительных шагов обработки данных, специфичных для ИТ-дискуссий. При решении поставленной задачи, именно работа~\cite{original} применялась в качестве основной. Данная работа также использует модель скрытого размещения Дирихле (LDA), котороя более подробно рассмотрена далее в этом разделе.

Работы~\cite{LDA1} и~\cite{LDA2} также используют LDA для вопросно-ответных систем. Работа~\cite{LDA1}, однако, предлагает проводить тематическое моделирование в рамках одного обращения, что может дать менее качественный результат, поскольку LDA показывает лучшие результаты на больших объемах данных. В \cite{LDA2} LDA используется для определения темы вновь поступивших вопросов, при этом для них не определяется ответ. В статьях~\cite{TMuse} и~\cite{2016lda} рассмотрены модификации LDA, призванные улучшить качество тематического моделирования.

Работа \cite{so} предлагает способ для нахождения лучшего ответа  на вопрос среди уже предоставленных на примере размеченных данных со Stack Overflow. В текущей работе не используются размеченные данные, что позволяет получить более универсальное решение. В статье \cite{engine} представлен другой подход поиска ответов, связанный с использованием поисковой системы. Сначала комментарии разделяются на 6 классов: вопрос, уточнение, ответ, отзыв на ответ, мусор. Затем используется специально настроенная поисковая система для поиска только по ответам. Основное отличие от текущей работы заключается в способе определения релевантных ответов.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Тематическое моделировлание}
\label{sec:overview_tm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Тематическое моделирование}~--- способ построения модели коллекции текстовых документов, которая определяет, к каким темам относится каждый из документов~\cite{TM}. 

Впервые задача тематического моделирования возникла в 1958 году, когда Герхард Лисовски и Леонард Рост завершили работу по составлению каталога религиозных текстов на иврите, которые должны были помочь учёным определить значения давно утраченых терминов. Затем они собрали вместе все возможные контексты, в которых появлялся каждый из терминов. Следующей задачей было научиться игнорировать несущественные различия в формах слов и выделять те различия, которые влияют на семантику. Замыслом авторов было дать возможность исследователям языка проанализировать различные отрывки и понять семантику каждого термина в его контексте. 

Проблемы такого рода возникают и сегодня при автоматическом анализе текстов. Одна и та же концепция может выражаться любым количеством различных терминов (синонимия), тогда как один термин часто имеет разные смыслы в различных контекстах (полисемия). Таким образом, необходимы способы различать варианты представления одной концепции и определять конкретный смысл многозначных терминов. Теоретически обоснованным и активно развивающимся направлением в анализе текстов на естественном языке, призванным решать перечисленные задачи, является тематическое моделирование коллекций текстовых документов.   

Построение тематической модели может рассматриваться как задача одновременной кластеризации документов и слов по одному и тому же множеству кластеров, называемых темами. В терминах кластерного анализа тема — это результат би-кластеризации, то есть одновременной кластеризации и слов, и документов по их семантической близости. Обычно выполняется нечёткая кластеризация, то есть документ может принадлежать нескольким темам в различной степени. Таким образом, сжатое семантическое описание слова или документа представляет собой вероятностное распределение на множестве тем. Процесс нахождения этих распределений называется тематическим моделированием. 

\textbf{Тематическая модель} (англ. topic model) коллекции текстовых документов определяет, к каким темам относится каждый документ и какие слова~(термины) образуют каждую тему~\cite{ML_PTM}. 

Переход из пространства терминов в пространство найденных тематик помогает эффективнее решать такие задачи, как тематический поиск, классификация и аннотация коллекций документов и новостных потоков.

Тематическое моделирование как вид статистических моделей для нахождения скрытых тем встреченных в коллекции документов, нашло свое применение в таких областях как машинное обучение и обработка естественного языка. Исследователи используют различные тематические модели для анализа текстов, текстовых архивов документов, для анализа изменения тем в наборах документов. В случае, когда документ относится к определенной теме, в документах посвященных одной теме можно встретить некоторые слова чаще других. 

Например: <<собака>> и <<кость>> встречаются чаще в документах про собак, «кошки» и «молоко» будут встречаться в документах о коошках, предлоги «и» и «в» будут встречаться в обеих тематиках. Обычно документ касается нескольких тем в разных пропорциях, таким образом, документ в котором 10\% темы составляют кошки, а 90\% темы про собак, можно предположить, что слов про собак в 9 раз больше. Тематическое моделирование отражает эту интуицию в математическую структуру, которая позволяет на основании изучения коллекции документов и исследования частотных характеристик слов в каждом документе, сделать вывод, что каждый документ это некоторый баланс тем.

Как правило, количество тем, встречающихся в документах, меньше количества различных слов во всем наборе. Поэтому скрытые переменные (темы) позволяют представить документ в виде вектора в пространстве скрытых (латентных) тем вместо представления в пространстве слов. В результате документ имеет меньшее число компонент, что позволяет быстрее и эффективнее его обрабатывать. Таким образом, тематическое моделирование также может использоваться в таком классе задач, как уменьшение размерности данных. Кроме того, найденные темы могут использоваться для семантического анализа текстов. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Методы построения тематической модели}
\label{sec:tm_techniques}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Задача извлечения скрытых тем из коллекции текстовых документов имеет множество применений. Помимо кластеризации и классификации документов, найденные темы могут применяться для определения релевантности документа заданной теме или запросу, определения тематического сходства документа с другими документами и их фрагментами, построения тематических профилей авторов, разбиения документа на тематически однородные фрагменты и т.д. 

Современные способы тематического моделирования находят применение в широком спектр приложений: 

\begin{itemize}
\item Кластеризация, классификация, ранжирование, аннотирование и суммаризация отчётов, научных публикаций, архивов докуметов и т.д.; 
\item Тематический поиск документов и связанных с ними объектов;
\item Фильтрация спама; 
\item Построение тематических профилей пользователей форумов, блогов и социальных сетей для поиска тематических сообществ и определения наиболее активных их участников; 
\item Анализ новостных потоков и сообщений из социальных сетей для определения актуальных событий реального мира и реакции пользователей на них.
\end{itemize}

Тематическое моделирование позволяет автоматически систематизировать и обрабатывать электронные архивы такого масштаба, который человек не в силах обработать.

С точки зрения поставленной в данной работе задачи, тематчиеское моделирование используется для построения тематической модели корпуса обращений в службу поддержки. Затем в рамках одного обращения определяется пара комментариев, наиболее близкая к соответсвующей данному обращению теме. Таким образом учитывается сходство терминологии между задаваемым вопросом и полученным ответом на него.

\subsection{Кластеризация и классификация}

Задача определения и отслеживания тем возникла в 1996-1997 годах. В работе~\cite{Allan98topicdetection} понятие темы тесно связано с понятием события: тема~--- это событие или действие вместе со всеми непосредственно связанными событиями и действиями. Задача заключается в извлечении событий из потока информации. 
Для представления документов принято пользоваться векторной моделью, в которой каждому слову сопоставляется вес в  соответствии с выбранной весовой функцией. Располагая таким представлением для всех документов, можно, например, находить расстояние между точками пространства и тем самым решать задачу подобия документов~--- чем ближе расположены точки, тем больше похожи соответствующие документы. Классическим методом назначения весов словам является TF-IDF:
$$
TFIDF(t,d,D)=TF(t,d)*IDF(t,D) \eqno(1)
$$
TF (term frequency)~--- нормализованная частота слова в тексте: 
\nomenclature{TF}{Term Frequency, нормализованная частота слова в тексте}
$$
TF(t,d)=\frac{freq(t,d)}{max_{w\in D}freq(w,d)} \eqno(2)
$$
Здесь $freq(t,d)$~--- число вхождений слова $t$ в документ $d$.

IDF (inverse document frequency)~--- обратная частота документов: 
\nomenclature{IDF}{Inverse Document Frequency, обратная частота документа}
$$
IDF(t,D)=log\frac{|D|}{|\{d\in D:t\in d\}|} \eqno(3)
$$
Здесь в числителе~--- количество документов в наборе, а в знаменателе~--- количество документов, в которых встречается слово $t$. В зависимости от решаемой задачи используются различные модификации TF-IDF.  

Для сравнения векторов документов в~\cite{Allan98topicdetection} применялись такие метрики, как косинус, дивергенция Кульбака-Лейблера и другие методы (взвешенная сумма компонентов документа, простые языковые модели). Всего существует более 70 способов расчёта похожести векторов~\cite{choi2010survey}. В~\cite{Allan98topicdetection} рассматривается два типа задач: обнаружение событий из набора данных за определенный период времени и обнаружение событий в режиме реального времени.

Первый тип задач заключается в разбиении исходных данных на группы, соответствующие событиям, а также в определении, описывает ли текстовый документ из набора какое-либо событие. Основной идеей всех решений было использование алгоритмов кластеризации (инкрементальная кластеризация, метод K-средних и др). При этом предполагается, что каждый кластер содержит документы, описывающие какое-либо событие. Задача второго типа~--- для нового документа определить, описывает ли он событие, которое уже встречалось в исходных данных. Для отслеживания событий использовались алгоритмы классификации (метод k-ближайших соседей, решающие деревья и др). Классификация производилась с использованием двух классов: YES~--- документ описывает событие, NO~--- не описывает.
 
Таким образом, в ранних исследованиях тема отождествлялась с событием. В реальной жизни тема может описывать иные сущности, а не только события. Поэтому в более поздних работах задачи определения событий и тем стали различаться. Еще один недостаток описанных методов в том, что анализируемые документы относятся только к одной теме или событию. Однако один документ может затрагивать несколько тем. К тому же, векторное представление документов не позволяет разрешать синонимию и полисемию терминов. 

Для решения перечисленных проблем было предложено рассматривать набор векторов терминов из документов как общую терм-документную матрицу и применять к ней особые разложения (метод LSI).

\nomenclature{LSI}{Latent Semantic Indexing, Латентно-семантическое индексирование}

\subsection{Латентно-семантическое индексирование}

В 80-е годы прошлого столетия стали активно развиваться системы информационного поиска по коллекциям документов разнообразной природы. Первыми были реализованы подходы, основанные на поиске точных совпадений частей документов с запросами пользователей. Довольно скоро, однако, стало очевидно различие между релевантностью (соответствием) документа запросу и точным совпадением их частей. Зачастую документы, релевантные запросу с точки зрения пользователя, не содержали терминов из запроса и поэтому не отображались в результатах поиска (проблема синонимии). 

С другой стороны, большое количество документов, слабо или вовсе не соответствующих запросу, показывались пользователю только потому, что содержали термины из запроса (проблема полисемии). Самым простым решением этих проблем кажется добавление к запросу уточняющих терминов для более точного описания интересующего контекста. Однако предположение о том, что индекс поисковой системы содержит все возможные уточняющие термины, на практике выполняется довольно редко. 

В 1988 г. был предложен метод латентно-семантического индексирования (latent semantic indexing, LSI), призванный повысить эффективность работы информационно-поисковых систем путём проецирования документов и терминов в пространство более низкой размерности, которое содержит семантические концепции исходного набора документов. 

Основная идея метода состоит в оценивании корреляции терминов путём анализа их совместной встречаемости в документах. К примеру, в коллекции всего 100 документов, содержащих термины «доступ» и/или «поиск». При этом только 95 из них содержат оба термина вместе. Логично предположить, что отсутствие термина «поиск» в документе с термином «доступ» ошибочно и возвращать данный документ по запросу, содержащему только термин «доступ». Подобные выводы можно делать не только из простой попарной корреляции терминов. 

С другой стороны, анализируя корреляцию терминов в запросе, можно более точно определять интересующий пользователя смысл основного термина и повышать позиции документов, соответствующих этому смыслу, в результатах поиска.
 
Таким образом, при латентно-семантическом индексировании документов задача состоит в том, чтобы спроецировать часто встречающиеся вместе термины в одно и то же измерение семантического пространства, которое имеет пониженную размерность по сравнению с оригинальной терм- документной матрицей, которая обычно довольно разрежена. Элементы этой матрицы содержат веса терминов в документах, назначенные с помощью выбранной весовой функции. В качестве примера можно рассмотреть самый простой вариант такой матрицы, в которой вес термина равен 1, если он встретился в документе (независимо от количества появлений), и 0 если не встретился (таблица~\ref{td_matr}).
 
\begin{table}[h]
\caption{Терм-документная матрица}
\label{td_matr}
\centering
\begin{tabular}{c|cccccc}
 & $d_1$ & $d_2$ & $d_3$ & $d_4$ & $d_5$ & $d_6$ \\
\hline
ship & 1 & 0 & 1 & 0 & 0 & 0 \\
boat & 0 & 1 & 0 & 0 & 0 & 0 \\
ocean & 1 & 1 & 0 & 0 & 0 & 0 \\
voyage & 1 & 0 & 0 & 1 & 1 & 0 \\
trip & 0 & 0 & 0 & 1 & 0 & 1 \\
\end{tabular}
\end{table}

Наиболее распространенный вариант LSI основан на использовании разложения терм-документной матрицы по сингулярным значениям~--- сингулярном разложении (Singular Value Decomposition, SVD). 

\nomenclature{SVD}{Singular Value Decomposition}

Согласно теореме о сингулярном разложении, любая вещественная прямоугольная матрица может быть разложена на произведение трех матриц:
$$
A=TSD^T \eqno(4)
$$
где матрицы $T$ и $D$~--- ортогональные, а $S$~--- диагональная матрица, элементы на диагонали которой называются \textit{сингулярными значениями} матрицы $A$. Такое разложение обладает особенностью: если в матрице $S$ оставить только $k$ наибольших сингулярных значений, а в матрицах $T$ и $D$ — только соответствующие этим значениям столбцы, то произведение получившихся матриц $S$, $T$ и $D$ будет наилучшим приближением исходной матрицы $A$ к матрице $\widehat{A}$ ранга $k$. 

Если в качестве матрицы $A$ взять терм-документную матрицу, то матрица $\widehat{A}$, содержащая только $k$ первых линейно независимых компонент $A$, отражает основную структуру различных зависимостей, присутствующих в исходной матрице. Структура зависимостей определяется весовыми функциями терминов.

Таким образом, каждый термин и документ представляются при помощи векторов в общем семантическом пространстве размерности $k$. Близость между любой комбинацией терминов и/или документов вычисляется при помощи скалярного произведения векторов. Для задач информационного поиска запрос пользователя рассматривается как набор терминов, который проецируется в семантическое пространство, после чего полученное представление сравнивается с представлениями документов в коллекции. Как правило, выбор $k$ зависит от поставленной задачи и подбирается эмпирически. Если выбранное значение $k$ слишком велико, то метод теряет свою мощность и приближается по характеристикам к стандартным векторным методам. Слишком маленькое значение $k$ не позволяет улавливать различия между похожими терминами или документами. 

Следующим этапом развития тематического моделирования стали подходы, позволяющие моделировать вероятности скрытых тем в документах и терминов в темах. В отличие от дискриминативных подходов (к которым относится LSI), в вероятностных подходах сначала задаётся модель, а затем с помощью терм-документной матрицы оцениваются её скрытые параметры, которые затем могут быть использованы для генерации моделируемых распределений. Из этого следуют преимущества вероятностного моделирования документов:

\begin{itemize}
\item Результаты работы представляются в терминах теории вероятностей и поэтому могут быть с минимальными затратами встроены в другие вероятностные модели и проанализированы стандартными статистическими методами; 
\item Новые порции входных данных не требуют повторного обучения модели; 
\item Вероятностные модели могут быть расширены путём добавления переменных, а также новых связей между наблюдаемыми и скрытыми переменными.
\end{itemize}

\subsection{Вероятностный латентно-семантический анализ}

\nomenclature{PLSI}{Probabilistic Latent Semantic Indexing, вероятностное латентно-семантическое индексирование}

Вероятностное латентно-семантическое индексирование (Probabilistic Latent Semantic Indexing, PLSI) является одной из первых вероятностных моделей тематического моделирования, предложенной Томасом Хоффманом в 1999 году. 
В основе PLSI лежит так называемая аспектная модель, которая связывает скрытые переменные тем $z\in Z=\{z_1,...,z_k\}$ с каждой наблюдаемой переменной — словом или документом. Таким образом, каждый документ может относиться к нескольким темам с некоторой вероятностью, что является отличительной особенностью этой модели по сравнению с подходами, не позволяющими вероятностного моделирования. 

PLSI использует следующий генеративный процесс: 

\begin{enumerate*}
\item Выбрать документ $d$ согласно распределению $p(d)$;
\item Выбрать тему $i\in \{1,...,k\}$ на основе распределения \\ \mbox{$\theta_{di}=p(z=i|d)$};
\item Выбрать слово $v$ — значение переменной $w$ на основе распределения $\phi_{iw}=p(w=v|z=i)$;
\end{enumerate*}

Совместная вероятностная модель над документами и словами определена следующим образом:
$$
P(d,w)=P(d)\sum_{z\in Z}P(w|z)P(z|d) \eqno(5)
$$
Несмотря на очевидные преимущества перед более ранними подходами, модель PLSI имеет следующие недостатки. Во-первых, она содержит большое число параметров, которое растет в линейной зависимости от числа документов. Как следствие, модель склонна к переобучению и неприменима к большим наборам данных. Во-вторых, невозможно вычислить вероятность документа, которого нет в наборе данных. В-третьих, отсутствует какая-либо закономерность при генерации документов из сочетания полученных тем. Данные недостатки устранены в модели LDA.

\subsection{Латентное размещение Дирихле}

Ввиду недостатков, которыми обладала модель PLSA, её использование было весьма ограниченным. Поэтому в~\cite{LDA} была предложена модель латентного размещения Дирихле (latent Dirichlet allocation, LDA), лишённая недостатков PLSA. В LDA предполагается, что каждое слово в документе порождено некоторой латентной темой, при этом в явном виде моделируется распределение слов в каждой теме, а также априорное распределение тем в документе. 

Темы всех слов в документе предполагаются независимыми. В LDA, как и в PLSA, документ может соответствовать не одной теме. Но LDA задаёт модель порождения как слов, так и документов, поэтому появляется дополнительная возможность оценивать вероятности документов вне текстовой коллекции с помощью алгоритма вариационного вывода и семплирования Гиббса~\cite{gibbs}. 

В отличие от PLSA, в LDA число параметров не увеличивается с ростом числа документов в коллекции. Многочисленные расширения модели LDA устраняют некоторые её ограничения и улучшают производительность для конкретных задач. 
Каждый документ генерируется независимо:

\begin{enumerate}
\item Случайно выбрать для документа $d$ его распределение по темам~$\theta_d$;
\item Для каждого слова в документе:
\begin{enumerate}
\item Случайно выбрать тему из распределения $\theta_d$, полученного на 1-м шаге;
\item Случайно выбрать слово из распределения слов  $\phi_t$ в выбранной теме~$t$.
\end{enumerate}
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%
LDA описывает каждую тему $t$, как вероятностное распределение по всем словам из входных данных ($\phi_t$). Каждый документ $d$ описывается вероятностным распределением по темам ($\theta_d$). Цель LDA - максимизировать функцию (6) путем оптимизации $\phi$ и $\theta$:
$$
P(\theta, \phi)=\prod_{t=1}^{T}P(\phi_t)\prod_{d=1}^{D}P(\theta_d)\prod_{w=1}^{W_d}P(Z_{d,w}|\theta_d)P(N_{t,w}|\phi_t) \eqno(6)
$$
где $T$~--- количество тем, $D$~--- количество документов, $W_d$~--- количество различных слов в документе $d$, $Z_{d,w}$~--- определяет принадлежность слова~$w$ к документу~$d$ и~$N_{t,w}$~--- принадлежность слова~$w$ к теме~$t$.

В модели LDA предполагается, что параметры $\theta$ и $\phi$ в свою очередь зависят от гиперпараметров $\alpha$ и $\beta$ соответственно и распределены следующим образом:~$\theta\sim Dir(\alpha)$, $\phi\sim Dir(\beta)$ где $\alpha$ и $\beta$~--- задаваемые вектора-параметры (гиперпараметры) распределения Дирихле (обычно $\alpha$ принимается равным $50/T$, a параметр $\beta=0.1$, увеличение $\beta$ ведёт к более разреженным тематикам).

Для оценки параметров LDA используется сэмплирование по Гиббсу, которое состоит в том, чтобы на каждом шаге фиксировать все переменные, кроме одной, и выбирать оставшуюся переменную согласно распределению вероятности этой переменной при условии всех остальных.

\subsection{Другие методы}

LDA на сегодняшний день является наиболее распространенным алгоритмом, подходящем для решения большинства задач. Однако, существует ряд специализированных моделей, которые коротко представлены ниже. Более подробно данные модели рассмотрены в~\cite{TM}.

\textit{Автор-тематическая модель} (author-topic model) представляет собой расширение LDA для совместного описания документов и авторов.

\textit{Скрытая тематическая модель гипертекста} (latent topic hypertext model, LTHM) описывает закон порождения ссылок в корпусе гипертекстов.

В рамках \textit{композитной модели HMM-LDA} строится совместное описание синтаксиса и семантики текста. Скрытая марковская модель (HMM) описывает локальные закономерности между соседними словами, тогда как модель LDA даёт глобальное тематическое описание документа в целом.

\textit{Модель соответствий} (correspondence LDA, Corr-LDA) была исходно предложена для решения задачи аннотирования изображений, когда каждому изображению из обучающей выборки ставится в соответствие некоторое множество слов, и требуется сгенерировать список слов, подходящих к новому, ещё не аннотированному изображению.

Аналогичная вероятностная модель \textit{labeled LDA} (LLDA) предложена в для мягкой кластеризации генов.

\textit{Иерархический процесс Дирихле} (Hierarchical Dirichlet Process, HDP) является Байесовской непараметрической моделью, которая может быть использована для тематического моделирования с потенциально бесконечным числом тем.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Сравнение тематических моделей}
\label{sec:tm_comparison}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

В таблице~\ref{tm_comp} приведено сравенение описаных ранее тематических моделей. В таблицу не включены различные модификации LDA, поскольку такие модели специфичны для отдельных задач и не интересны в контексте данной работы.

\begin{table}[h]
\caption{Сравнение тематических моделей}
\label{tm_comp}
\centering
\begin{tabular}{ C{1.7cm} | C{1.7cm} | L{3.2cm} | L{3.2cm} }
\hline
\textbf{Модель} & \textbf{Вероятн.} \textbf{модель} & \textbf{Преимущества} & \textbf{Недостатки} \\
\hline
Класт. и классиф. &%
-- &%
Может быть использована любая подходящая метрика &%
Не учитывается синонимия и полисемия \\
\hline
LSI &%
-- &%
Снимается проблема синонимии и полисемии &%
Низкая скорость работы. Результат зависит от подбора числа значимых терминов \\
\hline
PLSI &%
+ &%
Документ содержит более одной темы &%
Не подходит для определения тем в новых документах. \\
\hline
LDA &%
+ &%
Подходит для определения тем новых документов. Документ содержит более одной темы &%
---\\
\hline
\end{tabular}
\end{table}

Модель LDA является вероятностной моделью и позволяет определять вероятность принадлежности документа к теме. Данная модель также может использоваться для определения распределения тем для новых документов без повторного обучения. Воспользуемся этой моделью для решения поставленной задачи.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Вывод}
\label{sec:overview_concl}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

В ходе данного раздела были рассмотрены сущестсвующие методики извлечения часто задаваемых вопросов, описаны их преимущества и недостатки, приведены существующие научные работы в соотвествующей предметной области. Для решения поставленных в данной работе задач (раздел~\ref{chap:task}) был предложен подход, использующий тематическую модель текста. Этот метод позволяет получить более качественные результаты, поскольку, в отличие от остальных подходов, учитывает сходство терминологии между вопросом и ответом.

В разделе также рассмотрены существующие тематические модели, приводится их сравнение. Для исользования выбирается модель LDA, как наиболее эффективная на текущий момент.